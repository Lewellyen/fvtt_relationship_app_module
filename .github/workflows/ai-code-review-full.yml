name: AI Code Review - Full Project Analysis

on:
  workflow_dispatch:
    inputs:
      scope:
        description: "Analyse-Bereich"
        required: false
        default: "all"
        type: choice
        options:
          - all
          - src
          - templates
          - styles

jobs:
  ai-code-review-full:
    name: Full Project AI Analysis
    runs-on: ubuntu-latest

    permissions:
      contents: read  # Repository-Inhalt lesen (f√ºr git, gh api)
      issues: write   # Issues erstellen
      pull-requests: read  # PR-Informationen lesen

    steps:
      - name: Checkout code
        uses: actions/checkout@v6
        with:
          fetch-depth: 0 # Full history for better context

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: "20.x"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Cursor CLI
        id: cursor-setup
        run: |
          echo "Installing Cursor CLI..."
          curl https://cursor.com/install -fsS | bash || {
            echo "Failed to install Cursor CLI via script, trying alternative method"
            exit 1
          }

          # F√ºge alle m√∂glichen Pfade zum PATH hinzu
          echo "$HOME/.cursor/bin" >> $GITHUB_PATH
          echo "$HOME/.local/bin" >> $GITHUB_PATH

          # Pr√ºfe Installation
          if [ -f "$HOME/.cursor/bin/cursor-agent" ]; then
            echo "‚úÖ Cursor CLI installed at: $HOME/.cursor/bin/cursor-agent"
            ls -la "$HOME/.cursor/bin/cursor-agent"
          elif [ -f "$HOME/.local/bin/cursor-agent" ]; then
            echo "‚úÖ Cursor CLI installed at: $HOME/.local/bin/cursor-agent"
            ls -la "$HOME/.local/bin/cursor-agent"
          else
            echo "‚ö†Ô∏è Warning: cursor-agent not found in expected locations"
            echo "Searching..."
            find "$HOME" -name "cursor-agent" -type f 2>/dev/null | head -5 || true
          fi

          echo "Cursor CLI installation completed"

      - name: Determine files to analyze
        id: files-to-analyze
        run: |
          SCOPE="${{ github.event.inputs.scope || 'all' }}"
          echo "Analyzing scope: $SCOPE"

          # Finde alle relevanten Dateien basierend auf Scope
          case "$SCOPE" in
            "src")
              find src -type f \( -name "*.ts" -o -name "*.js" -o -name "*.svelte" \) ! -name "*.test.ts" ! -name "*.spec.ts" ! -path "*/node_modules/*" > files_to_analyze.txt
              ;;
            "templates")
              find templates -type f \( -name "*.hbs" -o -name "*.html" -o -name "*.handlebars" \) ! -path "*/node_modules/*" > files_to_analyze.txt
              ;;
            "styles")
              find styles -type f \( -name "*.css" -o -name "*.scss" -o -name "*.sass" \) ! -path "*/node_modules/*" > files_to_analyze.txt
              ;;
            "all"|*)
              find src templates styles -type f \( -name "*.ts" -o -name "*.js" -o -name "*.svelte" -o -name "*.css" -o -name "*.hbs" -o -name "*.html" \) ! -name "*.test.ts" ! -name "*.spec.ts" ! -path "*/node_modules/*" > files_to_analyze.txt
              ;;
          esac

          # Pr√ºfe ob Dateien gefunden wurden
          if [ ! -s files_to_analyze.txt ]; then
            echo "No files found for analysis"
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "file_count=0" >> $GITHUB_OUTPUT
          else
            FILE_COUNT=$(wc -l < files_to_analyze.txt | tr -d ' ')
            echo "Found $FILE_COUNT files to analyze"
            echo "skip=false" >> $GITHUB_OUTPUT
            echo "file_count=$FILE_COUNT" >> $GITHUB_OUTPUT
            echo "scope=$SCOPE" >> $GITHUB_OUTPUT

            # Zeige erste 20 Dateien als Info
            echo "Files to analyze (showing first 20):"
            head -20 files_to_analyze.txt
            if [ "$FILE_COUNT" -gt 20 ]; then
              echo "... and $((FILE_COUNT - 20)) more files"
            fi
          fi

      - name: Create analysis prompt
        if: steps.files-to-analyze.outputs.skip == 'false'
        run: |
          SCOPE="${{ steps.files-to-analyze.outputs.scope }}"
          FILE_COUNT="${{ steps.files-to-analyze.outputs.file_count }}"

          cat > /tmp/analysis-prompt.md << PROMPT_EOF
          Du arbeitest in einem GitHub Actions Runner.

          Die GitHub CLI ist als \`gh\` verf√ºgbar und √ºber \`GH_TOKEN\` authentifiziert. Git ist verf√ºgbar.
          Du hast vollst√§ndigen Lesezugriff auf das Repository.

          # Kontext:
          - Repo: ${{ github.repository }}
          - Branch: ${{ github.ref_name }}
          - Commit: ${{ github.sha }}
          - Scope: ${SCOPE}
          - Dateien zu analysieren: ${FILE_COUNT}

          # Deine Aufgabe:

          Du bist ein erfahrener Code-Reviewer f√ºr ein TypeScript/Foundry VTT Modul-Projekt.
          Analysiere das KOMPLETTE Projekt (Scope: ${SCOPE}) auf Code-Qualit√§t, Architektur-Konformit√§t und potentielle Probleme.

          **WICHTIG:** Nutze \`git\` und \`gh\` um selbst auf die Dateien zuzugreifen. Lies die Dateien direkt aus dem Repository.
          Du musst nicht Code hier im Prompt haben - du kannst alle Dateien selbst lesen!

          ## Projektkontext

          Das Projekt verwendet:
          - **Clean Architecture** mit 4 Schichten: Domain ‚Üí Application ‚Üí Infrastructure ‚Üí Framework
          - **Result-Pattern** statt Exceptions (siehe ADR-0001)
          - **SOLID-Prinzipien** durchg√§ngig
          - **Port-Adapter-Pattern** f√ºr Foundry VTT Version-Kompatibilit√§t
          - **Dependency Inversion Principle (DIP)**: Abh√§ngigkeiten nur zu Interfaces, nie zu Implementierungen

          ## Analyseschwerpunkte

          ### 1. SOLID-Prinzipien Pr√ºfung

          **Single Responsibility Principle (SRP):**
          - Hat jede Klasse/Interface nur eine Verantwortlichkeit?
          - Gibt es "God Classes" die zu viel tun?
          - Sind Methoden fokussiert auf eine Aufgabe?

          **Open/Closed Principle (OCP):**
          - Ist Code erweiterbar ohne Modifikation?
          - Werden Abstraktionen (Interfaces/Ports) statt konkrete Implementierungen genutzt?
          - Gibt es unn√∂tige switch/if-Ketten die durch Polymorphismus ersetzt werden k√∂nnten?

          **Liskov Substitution Principle (LSP):**
          - K√∂nnen Implementierungen ihre Interfaces vollst√§ndig ersetzen?
          - Gibt es Verletzungen des Vertrags (z.B. st√§rkere Pr√§-/Postconditions)?

          **Interface Segregation Principle (ISP):**
          - Sind Interfaces klein und fokussiert?
          - M√ºssen Implementierungen Methoden implementieren die sie nicht nutzen?
          - Gibt es gro√üe "Fat Interfaces" die aufgeteilt werden sollten?

          **Dependency Inversion Principle (DIP):**
          - Abh√§ngigkeiten nur zu Abstraktionen (Interfaces, Ports)?
          - Keine direkten Abh√§ngigkeiten zu konkreten Implementierungen?
          - Verwendung von DI-Container f√ºr Abh√§ngigkeiten?
          - Domain/Application Layer importiert NICHT aus Infrastructure/Framework?

          ### 2. Result/Either-Pattern Konformit√§t

          - Werden Funktionen die fehlschlagen k√∂nnen als \`Result<T, E>\` zur√ºckgegeben?
          - Werden Exceptions nur f√ºr unerwartete Fehler geworfen (Assertion Failures, Programming Errors)?
          - Werden Result-Werte korrekt behandelt (if/else checks, match(), andThen())?
          - Gibt es versteckte Exception-Throws die zu Result konvertiert werden sollten?
          - Nutzung von \`ok()\`, \`err()\`, \`map()\`, \`andThen()\`, \`match()\` Utilities?

          **Ausnahmen (Exceptions erlaubt):**
          - √ñffentliche API (\`module-api.ts\`): \`container.resolve()\` f√ºr externe Module
          - Unerwartete Fehler: Assertion Failures, Programming Errors

          ### 3. Clean Architecture Schichttrennung

          **Layer-Regeln:**
          - Domain Layer: Keine Foundry-Dependencies, nur Business-Logik, Port-Interfaces
          - Application Layer: Nutzt Domain Ports, keine direkten Foundry-Calls
          - Infrastructure Layer: Foundry Adapter, Port-Implementierungen, DI-Infrastruktur
          - Framework Layer: Bootstrap, Config, API Exposition

          **Dependency-Regel:**
          - √Ñu√üere Schichten ‚Üí innere Schichten ‚úÖ
          - Innere Schichten ‚Üí √§u√üere Schichten ‚ùå
          - Domain importiert NICHT aus Infrastructure/Framework
          - Application importiert NICHT aus Framework

          **Import-Pfade pr√ºfen:**
          - \`src/domain/\` darf NICHT importieren aus: \`src/infrastructure/\`, \`src/framework/\`
          - \`src/application/\` darf NICHT importieren aus: \`src/framework/\`
          - \`@/domain/\`, \`@/application/\` d√ºrfen keine Foundry-spezifischen Typen verwenden

          ### 4. Port-Adapter-Pattern

          - Werden Foundry API-Calls nur in Adapter-Schicht gemacht?
          - Ports als Interfaces definiert?
          - Version-spezifische Implementierungen in \`ports/v13/\`, \`ports/v14/\`?
          - Service-Wrapper nutzen Port-Selector f√ºr Version-Auswahl?
          - Lazy Instantiation f√ºr Ports (keine sofortige Instanziierung aller Versionen)?

          ### 5. Code Smells & Anti-Patterns

          **Code Smells:**
          - Long Methods (> 50 Zeilen)
          - Large Classes (> 500 Zeilen)
          - Feature Envy (Methoden nutzen mehr fremde als eigene Daten)
          - Data Clumps (Gruppen von Daten die zusammen geh√∂ren sollten)
          - Primitive Obsession (Strings/Numbers statt Value Objects)
          - Duplicate Code
          - Magic Numbers/Strings

          **Anti-Patterns:**
          - God Object / God Class
          - Swiss Army Knife Interface
          - Anemic Domain Model
          - Service Locator Pattern (au√üer in Config-Layer)
          - Tight Coupling
          - Circular Dependencies
          - Leaky Abstractions

          ### 6. Bugs & Fehlerquellen

          - Unbehandelte Fehler (Result nicht gepr√ºft)
          - Race Conditions (async/await Probleme)
          - Memory Leaks (Event Listeners nicht entfernt, Closures)
          - Null/Undefined Checks fehlen
          - Type Assertions ohne Validierung
          - Side Effects in pure functions
          - Mutable Shared State

          ## Output-Format

          **WICHTIG:** Du MUSST die Analyse-Ergebnisse AUSSCHLIESSLICH als g√ºltiges JSON ausgeben - KEIN Markdown, KEIN zus√§tzlicher Text vor oder nach dem JSON!

          Gib die Analyse-Ergebnisse als strukturiertes JSON aus. Fokussiere auf die WICHTIGSTEN und KRITISCHSTEN Probleme, da das Projekt gro√ü ist.
          Priorisiere:
          1. High-Severity Issues (DIP-Verst√∂√üe, Layer-Trennung, kritische Bugs)
          2. Medium-Severity Issues (SOLID-Verst√∂√üe, Result-Pattern-Missbrauch)
          3. Low-Severity Issues (Code Smells ohne direkten Impact)

          Beginne direkt mit dem JSON-Objekt, KEINE Markdown-Formatierung, KEIN Code-Block!

          {
            "summary": {
              "total_issues": 0,
              "files_analyzed": 0,
              "by_type": {
                "solid_violation": 0,
                "result_pattern_violation": 0,
                "architecture_violation": 0,
                "code_smell": 0,
                "bug": 0
              },
              "by_severity": {
                "critical": 0,
                "high": 0,
                "medium": 0,
                "low": 0
              }
            },
            "issues": [
              {
                "type": "solid_violation|result_pattern_violation|architecture_violation|code_smell|bug",
                "solid_principle": "SRP|OCP|LSP|ISP|DIP" (nur bei solid_violation),
                "severity": "critical|high|medium|low",
                "file": "src/path/to/file.ts",
                "line": 42,
                "column": 10,
                "title": "Kurze, pr√§gnante Beschreibung",
                "description": "Detaillierte Beschreibung des Problems",
                "current_code": "Relevanter Code-Ausschnitt (5-10 Zeilen)",
                "recommendation": "Konkreter Vorschlag zur Behebung mit Beispiel",
                "references": [
                  "ADR-0001",
                  "docs/architecture/event-system-hierarchy.md"
                ]
              }
            ]
          }

          **Severity-Kriterien:**
          - **critical**: Kritischer Bug mit hohem Runtime-Risiko, fundamentale Architektur-Verletzung
          - **high**: Versto√ü gegen fundamentale Prinzipien (DIP, Layer-Trennung), Bug mit Runtime-Risiko
          - **medium**: SOLID-Versto√ü, Result-Pattern-Missbrauch, Code Smell mit Wartbarkeits-Impact
          - **low**: Code Smell ohne direkten Impact, Verbesserungsvorschlag

          ## Wichtige Hinweise

          - Analysiere ALLE Dateien im bereitgestellten Scope (${FILE_COUNT} Dateien)
          - Fokussiere auf die KRITISCHSTEN Probleme (nicht jedes kleine Detail)
          - Sei pr√§zise und konkrete - vermeide vage Hinweise
          - Ber√ºcksichtige Projekt-spezifische Patterns (Result-Pattern ist Standard!)
          - Bevorzuge konstruktive Verbesserungsvorschl√§ge
          - Ignoriere bereits dokumentierte Ausnahmen (siehe quality-gates/)
          - Gruppiere √§hnliche Probleme wenn sinnvoll

          ## Zu analysierende Dateien

          Analysiere ALLE Dateien im Scope \`${SCOPE}\` (${FILE_COUNT} Dateien insgesamt).

          **So gehst du vor:**
          1. Nutze \`git ls-files\` oder \`gh api\` um alle relevanten Dateien zu finden
          2. Lies die Dateien mit \`cat\`, \`git show\`, oder \`gh api\`
          3. Analysiere den Code direkt aus dem Repository
          4. Du kannst auch \`git diff\`, \`git log\` nutzen f√ºr Kontext

          **Dateien-Filter (Scope: ${SCOPE}):**
          PROMPT_EOF

          # F√ºge Scope-Information hinzu (aber keine Code-Dateien!)
          if [ -f files_to_analyze.txt ]; then
            echo "" >> /tmp/analysis-prompt.md
            echo "**Hinweis:** Analysiere alle Dateien die zu diesem Scope geh√∂ren:" >> /tmp/analysis-prompt.md
            echo "" >> /tmp/analysis-prompt.md
            echo "\`\`\`" >> /tmp/analysis-prompt.md
            head -20 files_to_analyze.txt >> /tmp/analysis-prompt.md || true
            if [ "$(wc -l < files_to_analyze.txt | tr -d ' ')" -gt 20 ]; then
              echo "... und $(($(wc -l < files_to_analyze.txt | tr -d ' ') - 20)) weitere Dateien" >> /tmp/analysis-prompt.md
            fi
            echo "\`\`\`" >> /tmp/analysis-prompt.md
            echo "" >> /tmp/analysis-prompt.md
            echo "Diese Liste dient nur als Orientierung - du sollst ALLE relevanten Dateien selbst finden und analysieren!" >> /tmp/analysis-prompt.md
          fi

      - name: Run Cursor AI Analysis
        if: steps.files-to-analyze.outputs.skip == 'false'
        id: analysis
        env:
          CURSOR_API_KEY: ${{ secrets.CURSOR_API_KEY }}
          CURSOR_AI_MODEL: ${{ secrets.CURSOR_AI_MODEL || 'sonnet-4.5' }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true
        run: |
          echo "Starting full project Cursor AI analysis..."
          echo "Analyzing ${{ steps.files-to-analyze.outputs.file_count }} files"

          # Setze Standard-Modell falls nicht als Secret definiert
          CURSOR_AI_MODEL="${CURSOR_AI_MODEL:-sonnet-4.5}"
          echo "Using model: $CURSOR_AI_MODEL"

          # Prompt-Gr√∂√üe pr√ºfen
          PROMPT_SIZE=$(wc -c < /tmp/analysis-prompt.md)
          echo "Prompt size: $PROMPT_SIZE bytes"

          # Lese Prompt aus Datei und √ºbergebe direkt an cursor-agent (wie in der offiziellen Dokumentation)
          # Bash kann gro√üe Strings direkt an subprocess √ºbergeben
          echo "Reading prompt and calling cursor-agent directly..."
          PROMPT=$(cat /tmp/analysis-prompt.md)

          # Rufe cursor-agent direkt auf (wie im offiziellen Beispiel)
          # Python subprocess h√§tte auch funktioniert, aber direkter Aufruf ist einfacher
          # Timeout von 30 Minuten (1800 Sekunden) f√ºr gro√üe Analysen
          timeout 1800 cursor-agent -p "$PROMPT" --model "$CURSOR_AI_MODEL" > /tmp/analysis-output-raw.txt 2>&1 || {
            EXIT_CODE=$?
            if [ $EXIT_CODE -eq 124 ]; then
              echo "Analysis timed out after 30 minutes"
            else
              echo "Cursor AI analysis completed with exit code: $EXIT_CODE"
              echo "This may be normal - checking for JSON in output..."
            fi
          }
          EXIT_CODE=${EXIT_CODE:-0}

          # Kopiere Raw-Output f√ºr Debugging
          cp /tmp/analysis-output-raw.txt /tmp/analysis-output.json

          # Pr√ºfe ob Output existiert
          if [ ! -f /tmp/analysis-output.json ] || [ ! -s /tmp/analysis-output.json ]; then
            echo "No analysis output generated"
            echo "skipped=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Zeige Preview des Outputs (f√ºr Debugging)
          echo "Output preview (first 500 chars):"
          head -c 500 /tmp/analysis-output.json || true
          echo ""

          # Validierung: Pr√ºfe ob es bereits g√ºltiges JSON ist
          if python3 -m json.tool /tmp/analysis-output.json > /dev/null 2>&1; then
            echo "Output is valid JSON"
            echo "skipped=false" >> $GITHUB_OUTPUT
          else
            echo "Output is not valid JSON, attempting extraction..."
            if python3 scripts/ai-review-extract-json.py; then
              echo "JSON extraction successful"
              echo "skipped=false" >> $GITHUB_OUTPUT
            else
              echo "‚ö†Ô∏è Failed to extract valid JSON from output"
              echo "Raw output saved for inspection"
              echo "skipped=true" >> $GITHUB_OUTPUT
              # Workflow darf weiterlaufen, aber keine Issues erstellen
            fi
          fi

      - name: Parse results and create issues
        if: steps.files-to-analyze.outputs.skip == 'false' && steps.analysis.outputs.skipped == 'false'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_SHA: ${{ github.sha }}
        run: |
          # Erstelle Label f√ºr Full-Review (falls nicht vorhanden)
          python3 scripts/ai-review-create-label.py

          # Erstelle Issues
          python3 scripts/ai-review-create-issues.py

      - name: Create summary
        if: always()
        run: |
          echo "## üîç Full Project AI Code Review" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.files-to-analyze.outputs.skip }}" == "true" ]; then
            echo "‚è≠Ô∏è Keine Dateien gefunden - Analyse √ºbersprungen" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.analysis.outputs.skipped }}" == "true" ]; then
            echo "‚ö†Ô∏è Analyse konnte nicht ausgef√ºhrt werden (keine g√ºltigen Ergebnisse oder Timeout)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Dateien analysiert:** ${{ steps.files-to-analyze.outputs.file_count }}" >> $GITHUB_STEP_SUMMARY
            echo "**Scope:** ${{ steps.files-to-analyze.outputs.scope }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Scope:** ${{ steps.files-to-analyze.outputs.scope }}" >> $GITHUB_STEP_SUMMARY
            echo "**Dateien analysiert:** ${{ steps.files-to-analyze.outputs.file_count }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            if [ "${{ steps.analysis.outputs.skipped }}" == "true" ]; then
              echo "‚ö†Ô∏è **Analyse-Output konnte nicht verarbeitet werden**" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "M√∂gliche Ursachen:" >> $GITHUB_STEP_SUMMARY
              echo "- Cursor AI hat kein JSON zur√ºckgegeben" >> $GITHUB_STEP_SUMMARY
              echo "- JSON-Extraktion fehlgeschlagen" >> $GITHUB_STEP_SUMMARY
              echo "- Timeout bei der Analyse" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "Bitte pr√ºfe die Workflow-Logs f√ºr Details." >> $GITHUB_STEP_SUMMARY
            elif [ -f /tmp/analysis-output.json ]; then
              python3 scripts/ai-review-summary.py >> $GITHUB_STEP_SUMMARY
            else
              echo "‚ö†Ô∏è Keine Analyse-Ergebnisse verf√ºgbar" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow:** ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** \`${{ github.ref_name }}\`" >> $GITHUB_STEP_SUMMARY
